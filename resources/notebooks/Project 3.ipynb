{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural IBM1\n",
    "\n",
    "\n",
    "Conside IBM1's graphical model below, you will be replacing the standard parameterisation with tabular CPDs by deterministic parametric functions.\n",
    "\n",
    "![ibm1](img/ibm1.png)\n",
    "\n",
    "**Variables:**\n",
    "\n",
    "* \\\\(F\\\\) is a Categorical random variable taking values from the French vocabulary\n",
    "* \\\\(E\\\\) is a Categorical random variable taking values from the English vocabulary (extended by a NULL token)\n",
    "* \\\\(A\\\\) is a Categorical random variable taking values in the set \\\\(0, \\ldots, m\\\\), we call this variable *alignment* is it selects a mixture component (English type) in the English sentence that is used to generate a French word\n",
    "* \\\\(\\theta\\\\) is a set of deterministic parameter assignments\n",
    "* throughout, we assume \\\\(m\\\\) (the length of the English sentence) to be random, but observed\n",
    "\n",
    "We can now write the joint distribution in terms of the conditional probability distributions (CPDs) in this directed graphical model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">***Complete this***</span>\n",
    "\n",
    "\\begin{align}\n",
    "P_\\theta(f_1^n, a_1^n|e_0^m) &= \\prod_{j=1}^n P_\\theta(f_j, a_j|e_0^m) \\\\\n",
    " &= \\ldots\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameterisation\n",
    "\n",
    "Throughout, we will assume \\\\(P(A|m)\\\\) to always distribute uniformly over \\\\(m+1\\\\) events.\n",
    "In this project we will concentrate on the lexical distribution (but you can probably imagine how to extend the argument).\n",
    "\n",
    "IBM1 is parameterised by tabular CPDs, that is, tables of independent (up to a normalisation) probabilities values, where we have one value for each condition-outcome pair.\n",
    "\n",
    "**Tabular CPD:**\n",
    "\n",
    "\\begin{align}\n",
    "P(F|E=e) &= \\mathrm{Cat}(\\theta_e) \\\\\n",
    " &\\quad \\text{where } 0 \\le \\theta_{f|e}\\le 1 \\\\\n",
    " &\\quad \\text{ and } \\sum_{f \\in V_F} \\theta_{f|e} = 1\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "* one parameter \\\\(\\theta_{f|e}\\\\) per lexical event\n",
    "* parameters are stored in a table\n",
    "\n",
    "But nothing prevents us from using other parameterisations, for example, a feed-forward network would allow some parameters to be shared across events.\n",
    "\n",
    "**Feed-forward neural network:**\n",
    "* \\\\(P(F|E=e) = \\mathrm{Cat}(f_\\theta(e))\\\\) where \n",
    "    * \\\\(f_\\theta(e) = \\mathrm{softmax}(W_f r(e) + b_f)\\\\)\n",
    "        * note that the softmax is necessary to make \\\\(f_\\theta\\\\) produce valid parameters for the categorical distribution\n",
    "        * \\\\(W_f \\in \\mathbb R^{V_F \\times d}\\\\) and \\\\(b_f \\in \\mathbb R^{V_F}\\\\) \n",
    "        * \\\\(r(e) = W_r v(e)\\\\) with \\\\(W_r \\in \\mathbb R^{d \\times V_F}\\\\) \n",
    "        * \\\\(v(e) \\in \\{0,1\\}^{V_E} \\\\) is a one-hot encoding of \\\\(e\\\\), thus \\\\(\\sum_i v(e)_i = 1\\\\) \n",
    "    * \\\\(\\theta = \\{W_f, b_f, W_r\\}\\\\)\n",
    "\n",
    "Other architectures are also possible, one can use different parameterisations that may use more or less parameters. For example, with a CNN one could make this function sensitive to characters in the words, something along these lines could also be done with RNNs. We will use FFNNs in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE\n",
    "\n",
    "We can use maximum likelihood estimation (MLE) to choose the parameters of our deterministic function \\\\(f_\\theta\\\\). We know at least one general (convex) optimisation algorithm, i.e. *gradient ascent*. This is a gradient-based procedure which chooses \\\\(\\theta\\\\) so that the gradient of our objective with respect to \\\\(\\theta\\\\) is zero. Even though gradient ascent is meant for convex functions, we often apply it to nonconvex problems. IBM1 would be convex with standard tabular CPDs, but FFNNs with 1 nonlinear hidden layer (or more) make it nonconvex.\n",
    "\n",
    "Nowadays, we have tools that can perform automatic differentiation for us. Thus, for as long as our functions are differentiable, we can get gradients for them rather easily. This is convenient because we get to abstract away from most of the analytical work.\n",
    "\n",
    "Still, some analytical work is on us when working with latent variable models. For example, we still need to be able to express the functional form of the likelihood.\n",
    "\n",
    "Let us then express the log-likelihood (which is the objective we maximise in MLE) of a single sentence pair as a function of our free parameters (it generalises trivially to multiple sentence pairs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">***Complete this***</span>\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathcal L(\\theta|e_0^m, f_1^n) &= \\log P_\\theta(F_1^n=f_1^n|E_0^m = e_0^m) \\\\\n",
    "    &= \\ldots\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in fact our log-likelihood is a sum of independent terms \\\\(\\mathcal L_j(\\theta|e_0^m, f_j)\\\\), thus we can characterise the contribution of each French word in each sentence pair as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">***Complete this***</span>\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal L_j(\\theta|e_0^m, f_j) &= \\log P_\\theta(F=f_j|E_0^m = e_0^m) \\\\\n",
    " &= \\ldots \n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network toolkits usually implement several flavours of gradient-based optimisation for us. But, they are mostly designed as *minimisation* (rather than *maximisation*) algorithms. Thus, we have to work with the idea of a *loss*.\n",
    "\n",
    "To get a loss, we simply negate our objective. \n",
    "\n",
    "You will find a lot of material that mentions some *categorical cross-entropy loss*. \n",
    "\n",
    "\\begin{align}\n",
    "    l(\\theta) &= -\\sum_{(e_0^m, f_1^n)} p_\\star(f_1^n|e_0^m) \\log P_\\theta(f_1^n|e_0^m) \\\\\n",
    "    &\\approx -\\frac{1}{S} \\log P_\\theta(f_1^n|e_0^m)\n",
    "\\end{align}\n",
    "\n",
    "But see that this is just the likelihood of our data assuming that the observations were independently sampled from the true data generating process \\\\(p_\\star\\\\).\n",
    "\n",
    "As discussed above, due to the assumptions in our graphical model, this loss factors over individual French positions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">***Complete this***</span>\n",
    "\n",
    "\\begin{align}\n",
    "    l(\\theta|\\mathcal D) &= -\\frac{1}{S} \\sum_{(e_0^m, f_1^n) \\in \\mathcal D} \\sum_{j}^n \\mathcal L_j(\\theta|e_0^m, f_j) \\\\\n",
    "    &= \\ldots\n",
    "\\end{align}\n",
    "\n",
    "Here \\\\(\\mathcal D\\\\) is our dataset of \\\\(S\\\\) sentence pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD\n",
    "\n",
    "SGD is really quite simple, we sample a subset \\\\(\\mathcal S\\\\) of the training data and compute a loss for that sample. We then use automatic differentiation to obtain a gradient \\\\(\\nabla_\\theta \\mathcal l(\\theta|\\mathcal S)\\\\). This gradient is used to update our deterministic parameters \\\\(\\theta\\\\).\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\theta^{(t+1)} &= \\theta^{(t)} - \\delta_t \\nabla_{\\theta^{(t)}} l(\\theta^{(t)}|\\mathcal S)\n",
    "\\end{align}\n",
    "\n",
    "The key here is to have a learning rate schedule that complies with a [Robbins and Monro](https://www.jstor.org/stable/2236626) sequence (check [this](http://cilvr.cs.nyu.edu/diglib/lsml/bottou-sgd-tricks-2012.pdf) for practical notes).\n",
    ".\n",
    "Stochastic optimisers are very well studied. Neural network toolkits implement several *well defined* optimisers for you, so using a valid learning rate sequence should not represent much work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching (and notes on terminology)\n",
    "\n",
    "In neural network terminology \\\\(f_1, \\ldots, f_n\\\\) is a sample and \\\\(j\\\\) is a timestep. A collection of samples is a batch. We often work with collections of samples that are much smaller than the full dataset. \n",
    "\n",
    "A note on implementation: \n",
    "* most toolkits deal with static computational graphs, thus we typically have to batch sequences of fixed length (which may require some padding);\n",
    "* padding essentially means that sometimes we will be performing useless computations associated with samples that are not really there, in which case we will be masking (setting to 0) their contributions to the loss as to avoid learning from them.\n",
    "\n",
    "We are providing a tensorflow implementation of this basic neural extension to IBM1.\n",
    "Your task will be to extend it further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extensions\n",
    "\n",
    "From here we will discuss a few extensions which you will experiment with.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural IBM1 (with monolingual context)\n",
    "\n",
    "\n",
    "Consider the following extension:\n",
    "\n",
    "![ibm1](img/ibm1prev.png)\n",
    "\n",
    "\n",
    "Now that we can use FFNNs to deterministically predict the parameters of our categorical distributions, we can afford conditioning on more events! This model for example generates a French word at position \\\\(j\\\\) by conditioning on two events:\n",
    "1. the English word that occupies the position it aligns to\n",
    "2. and the French word in the previous position\n",
    "\n",
    "Let us start by writing the joint distribution, but let us show it for a single French word position (since we can generalise for a whole sentence trivially):\n",
    "\n",
    "\\begin{align}\n",
    "P_\\theta(f_j|e_0^m) &= \\sum_{a_j=0}^m P(f_j, a_j|e_0^m, f_{j-1}) \\\\\n",
    " &= \\sum_{a_j=0}^m P(a_j|m) P(f_j|e_{a_j}, f_{j-1}) \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using tabular CPDs, this would be difficult to model, there would be \\\\(v_F \\times v_E\\\\) CPDs, each of which defined over \\\\(v_F\\\\) events. Thus, a lot of free parameters.\n",
    "\n",
    "We are going to use FFNNs and make \n",
    "\n",
    "\\begin{equation}\n",
    "P(F|E=e, F_{\\text{prev}}=f_{\\text{prev}}) = \\mathrm{Cat}(g_\\theta(e, f_{\\text{prev}}))\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">***Complete this***</span>\n",
    "\n",
    "Let's use both observations by concatenating their word embeddings.\n",
    "\n",
    "1. embed e\n",
    "2. embed the previous f\n",
    "3. pass the concatenated embedding through affine transformation and nonlinearity\n",
    "4. predict categorical parameters (i.e. affine transformation followed by softmax)\n",
    "\n",
    "\n",
    "* \\\\(g_\\theta(e, f) = \\mathrm{softmax}(W_g r(e, f) + b_g)\\\\)\n",
    "    * \\\\(W_f \\in \\mathbb R^{V_F \\times d}\\\\) and \\\\(b_f \\in \\mathbb R^{V_F}\\\\) \n",
    "    * ...\n",
    "* \\\\(\\theta = \\{W_g, b_g, \\ldots\\}\\\\)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">***Complete this***</span>\n",
    "\n",
    "Let's use both observations by summing nonlinear transformations of their word embeddings scaled by a gate value (a scalar between 0 and 1).\n",
    "\n",
    "1. embed e \n",
    "2. embed the previous f\n",
    "3. compute a gate value (a number between 0 and 1) from the embedding of the previous word\n",
    "4. compute a nonlinear transformation of the embedding of e\n",
    "5. compute a nonlinear transformation of the embedding of the previous f\n",
    "6. combine both representations with a weighted sum, where the representation of the previous f gets weighted by the gate value, and the representation of e is weighted by 1 minus the gate value\n",
    "5. from the resulting vector, predict the parameters of the Categorical (that is, affine transformation followed by softmax)\n",
    "\n",
    "\n",
    "* \\\\(g_\\theta(e, f) = \\mathrm{softmax}(W_g r(e, f) + b_g)\\\\)\n",
    "    * \\\\(W_f \\in \\mathbb R^{V_F \\times d}\\\\) and \\\\(b_f \\in \\mathbb R^{V_F}\\\\) \n",
    "    * ...\n",
    "* \\\\(\\theta = \\{W_g, b_g, \\ldots\\}\\\\)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">***Complete this***</span>\n",
    "\n",
    "Discuss the differences between the two parameterisations above and the role of a gate value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural IBM1 with collocations\n",
    "\n",
    "Consider the following extension:\n",
    "\n",
    "![ibm1](img/ibm1c.png)\n",
    "\n",
    "where we have introduce a binary latent variable \\\\(c\\\\) which decides between English components and French components. That is, when \\\\(c=0\\\\) we generate a French word by *translating* an English word, when \\\\(c=1\\\\) we generate a French word by *inserting* it from monolingual (French) context.\n",
    "\n",
    "**Note**:\n",
    "* In comparison to the standard IBM1, French words are now themselves components, and they become available as we progress generating the French string from left-to-right.\n",
    "* In comparison to the previous extension (IBM1 with monolingual context), we incorporate a different type of inductive bias as we give the model the power to explicitly choose between English and French components.\n",
    "* Because we have an explicit latent treatment of this collocation variable, the model will reason with all of its possible assignments. That is, we will effectively marginalise over all options when computing the likelihood of observations (just like we did for alignment).\n",
    "\n",
    "\n",
    "This is the marginal likelihood  (for a single French word position):\n",
    "\n",
    "\\begin{align}\n",
    "P_\\theta(f_j|e_0^m) &= \\sum_{a_j=0}^m P(a_j|m) \\left( \\sum_{c_j=0}^1 P(c_j|f_{j-1})P(f_j|e_{a_j}, f_{j-1}) \\right)\\\\\n",
    " &= \\sum_{a_j=0}^m P(a_j|m) \\left(P(C_j=0|F_{j-1}=f_{j-1})P(F=f_j|E=e_{a_j}) + P(C_j=1|F_{j-1}=f_{j-1})P(F=f_j|F_{j-1}=f_{j-1})\\right)\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should be able to use simple FFNNs to parameterise the distributions\n",
    "* \\\\(P(C|F_{\\text{prev}})\\\\)\n",
    "* \\\\(P(F|F_{\\text{prev}})\\\\)\n",
    "* \\\\(P(F|E)\\\\)\n",
    "\n",
    "\n",
    "<span style=\"color:red\">***Complete this***</span>\n",
    "\n",
    "\\begin{equation}\n",
    "P(C|F_{\\text{prev}}=f_{\\text{prev}}) = \\mathrm{Bernoulli}(\\mathrm{sigmoid}(\\ldots))\n",
    "\\end{equation}\n",
    "\n",
    "* note that our FFNN will predict a single number which is the parameter of the Bernoulli, this parameter should be a single number between 0 and 1, that's why we use a sigmoid\n",
    "\n",
    "* ...\n",
    "\n",
    "\\begin{equation}\n",
    "P(F|F_{\\text{prev}}=f_{\\text{prev}}) = \\textrm{Cat}(\\ldots)\n",
    "\\end{equation}\n",
    "\n",
    "* ...\n",
    "\n",
    "\\begin{equation}\n",
    "P(F|E=e) = \\textrm{Cat}(\\ldots)\n",
    "\\end{equation}\n",
    "\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard NN models would use a *deterministic* gate value in place of this random collocation indicator. \n",
    "\n",
    "For a deterministic we do not have any marginalisation to compute. It also has weaker inductive bias. However, at least in MT, there is a lot of empirical evidence that somehow *soft* decisions (such as this blend of translation and insertion given by a gate value) performs better than *hard* decisions (as the discrete decision of either inserting or translating). We will see a *stochastic* extension of the gate value pretty soon.\n",
    "\n",
    "For now, can you comment on pros/cons of the stochastic view with discrete random variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">***Complete this***</span>\n",
    "\n",
    "1. Pros:\n",
    "2. Cons:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian IBM1 with collocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider this last extension:\n",
    "\n",
    "![ibm1](img/ibm1cz.png)\n",
    "\n",
    "here we made the Bernoulli parameter \\\\(z\\\\) a random variable.\n",
    "\n",
    "The big difference is that the CPD is a Bernoulli:\n",
    "\n",
    "\\begin{equation} \n",
    "P(C|F_{\\text{prev}}=f_{\\text{prev}}) = \\mathrm{Bernoulli}(z_{f_{\\text{prev}}})\n",
    "\\end{equation}\n",
    "\n",
    "whose parameter is distributed by a [Beta distribution](https://en.wikipedia.org/wiki/Beta_distribution) with fixed parameters \\\\(a\\\\) and \\\\(b\\\\), that is\n",
    "\n",
    "\\begin{equation}\n",
    "Z_{f_{\\text{prev}}} \\sim \\mathrm{Beta}(a,b)\n",
    "\\end{equation}\n",
    "\n",
    "We now have a big problem: our likelihood is no longer tractable!\n",
    "\n",
    "\\begin{align}\n",
    "P(f_j|e_0^m) &= \\sum_{a_j=0}^m P(a_j|m) \\int \\mathrm{Beta}(z_{f_{j-1}}|a,b) \\sum_{c_j=0}^1 P(c_j|z_{f_{j-1}}) P(f_j|e_{a_j}, c_j) \\mathrm{d}z_{f_{j-1}}\n",
    "\\end{align}\n",
    "\n",
    "it involves marginalising over all possible Bernoulli parameters.\n",
    "Before, this wasn't the case because we had a FFNN deterministically predict a single value for that parameter. Now we want to reason with all possible values of that parameter assuming a Beta prior.\n",
    "\n",
    "Because this is intractable, we will work with a *variational auto-encoder* (VAE).\n",
    "As discussed in class, with a simpler variational approximation that approximates the posterior, we can get an unbiased Monte Carlo (MC) estimate of the likelihood and gradient. \n",
    "In class, we saw a VAE that employed a Gaussian random variable, for which we derived a change of variable (*reparameterisation trick*). \n",
    "That trick is quite different for Beta distributions.\n",
    "\n",
    "<span style=\"color:red\">***Complete this***</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
