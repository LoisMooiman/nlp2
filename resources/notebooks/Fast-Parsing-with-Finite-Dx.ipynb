{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finite D(x) with constrained ITG\n",
    "\n",
    "For the standard ITG \n",
    "\n",
    "```\n",
    "S -> X\n",
    "X -> [X X]     (Monotone)\n",
    "X -> <X X>     (Inverted)\n",
    "X -> x/y       (Translation)\n",
    "X -> x/eps     (Deletion)\n",
    "X -> eps/y     (Insertion)\n",
    "```\n",
    "\n",
    "\\\\( D(x) = \\\\{ d: yield_\\Sigma(d)=x \\\\}  \\\\) \n",
    "is an infinite set. That's so because the insertion rule allows strings to grow unboundedly by mapping nothing in the source to a target word in the  lexicon.\n",
    "\n",
    "We've seen how to constrain that set by explicitly intersecting it with a finite regular language. Here we'll show you a different approach. We will modify the grammar so that D(x) is finite by construction. This is convenient because in training a CRF based on such modified grammar, we will be able to save some calls to a general parsing/intersection procedure.\n",
    "\n",
    "The approach is to first hard-code in the grammar whether an insertion happened. So we will define terminal rules whose nonterminals encode the purpose of the rule:\n",
    "\n",
    "```\n",
    "T -> x/y\n",
    "D -> x/eps\n",
    "I -> eps/y\n",
    "```\n",
    "\n",
    "We have then translation rules (T) where neither x nor y are empty, deletion rules (D) and insertion rules (I).\n",
    "\n",
    "Then we will upgrade these rules to the status of phrases (X):\n",
    "\n",
    "```\n",
    "X -> T\n",
    "X -> D\n",
    "X -> I\n",
    "```\n",
    "\n",
    "And allow phrases to be concatenated in either order recursively:\n",
    "\n",
    "```\n",
    "X -> [X X]\n",
    "X -> <X X>\n",
    "```\n",
    "\n",
    "Finally, phrases eventually become sentences:\n",
    "\n",
    "```\n",
    "S -> X\n",
    "```\n",
    "\n",
    "Now the crucial part. First we deal with the fact that we can delete as many words as we like by adding a recursive deletion: \n",
    "\n",
    "```\n",
    "D -> [D D]     \n",
    "```\n",
    "(Note there's no need for inverted as on the target side this would produce two empty strings either way -- which is not something sensitive to change in word order)\n",
    "\n",
    "But, we will not do the same for insertions! Instead, we will limit insertions to be accompanied by a translation (on either side):\n",
    "\n",
    "```\n",
    "X -> [T I]\n",
    "X -> <T I>\n",
    "X -> [I T]\n",
    "X -> <I T>\n",
    "```\n",
    "\n",
    "And this is it! An ITG for which the set D(x) is finite! Here is the complete grammar:\n",
    "\n",
    "```\n",
    "S -> X\n",
    "X -> [X X]\n",
    "X -> <X X>\n",
    "X -> T\n",
    "X -> D\n",
    "X -> I\n",
    "X -> [T I]\n",
    "X -> <T I>\n",
    "X -> [I T]\n",
    "X -> <I T>\n",
    "D -> [D D]\n",
    "T -> x/y\n",
    "D -> x/eps\n",
    "I -> eps/y\n",
    "```\n",
    "\n",
    "For you to try this out, we created two helper functions:\n",
    "\n",
    "1. `libitg.make_source_side_finite_itg`\n",
    "2. `libitg.make_target_side_finite_itg`\n",
    "\n",
    "They are drop in replacements for the older (standard ITG) version.\n",
    "\n",
    "Parsing with this grammar is much faster and we managed to parse longer sentences (We tested with as many as 20 words). You might even be able to play with a slightly bigger lexicon, but we make no promises ;). Check the file `fast_example.py` in the repository.\n",
    "\n",
    "**Importantly!** There's no need for \\\\(D_n(x)\\\\) *whatsoever*, so the roadmap would become:\n",
    "\n",
    "1.\tGet a grammar \n",
    "    `src_grammar = libitg.make_source_side_finite_itg(...)`\n",
    "2.\tParse the source\n",
    "    `_Dx = earley with src_grammar and src_fsa`\n",
    "3.\tProject the forest getting a finite \\\\(D(x)\\\\)\n",
    "    `Dx = libitg.make_source_target_finite_itg using _Dx`\n",
    "4.\tFor training, also parse the target using the newly obtained \\\\(D(x)\\\\)\n",
    "    `Dxy = earley with Dx and tgt_fsa`\n",
    "\n",
    "### Extra note on feature function\n",
    "\n",
    "The following paragraph was a bit confusing:\n",
    "\n",
    "> You need to make sure your feature function is aware that now you will have a single span per symbol when dealing with \\\\(D(x)\\\\) and a pair of spans when dealing with \\\\(D(x,y)\\\\). Also, your feature function can now capitalise on the new types of rules (for example to count number of translations, insertions and deletions in each edge).\n",
    "\n",
    "This might have implied that you need a separate feature function for \\\\(D(x, y)\\\\), but that is **not** the case. \\\\(D(x, y)\\\\) is just \\\\(D(x)\\\\) constrained.\n",
    "\n",
    "To be able to restrict \\\\(D(x)\\\\)  we need to transfer memory from the automaton to the grammar. The way intersection transfers memory is by \"refining\" nonterminals as to make them contain enough information to partition the set of strings they project on to. That's why D(x,y) has more span information. Symbols in there tell you about paths on top of the target FSA.\n",
    "\n",
    "**So quite formally, you are always featurising edges in \\\\(D(x)\\\\).** Every edge in \\\\(D(x, y)\\\\) is nothing but an edge of \\\\(D(x)\\\\) with extra span information (from where you should not really get any additional features).\n",
    "\n",
    "Thus the right way to go about designing your feature function is to **design it once**. And it should be designed for D(x). Your log-potentials are now  \\\\(w \\cdot \\phi(r, s, x)\\\\) where r is the rule identity, s the source span, and x the source sentence. Compared to what we had before with \\\\(D_n(x)\\\\), we lost access to t (the target span), but hopefully you will see below that we haven't lost all that much.\n",
    "\n",
    "### Why losing access to the target span does not matter a lot\n",
    "\n",
    "Before, you couldn't really get much from the target span. For example, you couldn't use target span info to make features that would inspect target n-grams (because y was not really available and having those features would break the edge factorisation of the model: think of it this way, you wouldn't be able to get target n-gram features from \\\\(D_n(x)\\\\), and whatever you can get from \\\\(D(x,y)\\\\) only is useless because you will need to work with both when doing MLE).\n",
    "\n",
    "The only thing you could really get from target span info was span size. And that's somewhat gone now (it's the price we pay for not intersecting an automaton that can count string length for us). Also note this: figuring length is indeed expensive, it makes the forest much bigger because you need to refine nonterminals as to have them encode the size of the strings they project onto. That's where the current improved performance comes from: our grammar manages to constrain insertions without counting directly.\n",
    "\n",
    "Now, some of the target length information you can actually bootstrap without access to target span info directly. You can, locally to each edge in D(x), have a feature that counts whether translation/insertion/deletion happened, because LHS symbols like T/D/I inform you of that. Since this is a global model, the feature \"insertion\" for a certain derivation will be the sum of \"insertion\" along each edge, which will give you the number of insertions. So if you increment a feature like \"target-length\" each time either translation or insertion happens in an edge, globally for a path that will be correct.\n",
    "\n",
    "**In sum, design one feature function only and do it limiting yourself to the degree of information a available in edges of D(x).** When dealing with D(x,y) just ignore the target span info and focus on the common part: r, s, and x.\n",
    "\n",
    "In practical terms, if, for a training/test instance, you **hash** your features by (r,s) you'll not make mistakes. And in training, after featurising edges in D(x), you'll have all quantities necessary for D(x,y) already cached.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "We hope this helps everybody discard less data and parse quickly!\n",
    "\n",
    "Note that sometimes it's normal that this grammar produces an empty \\\\(D(x,y)\\\\) because some strings are not within its generative capacity. In those cases you can simply discard the training instance. \n",
    "\n",
    "If you go with this new grammar, note that effectively you will be using \\\\(D(x)\\\\) everywhere where before you would be using \\\\(D_n(x). \\\\)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fast_example import test\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEXICON\n",
      "-EPS-: 5 options\n",
      "noir: 5 options\n",
      "le: 5 options\n",
      "e: 5 options\n",
      "petite: 5 options\n",
      ".: 5 options\n",
      "chien: 5 options\n",
      "petit: 5 options\n",
      "blanc: 5 options\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create test lexicon\n",
    "lexicon = defaultdict(set)\n",
    "lexicon['le'].update(['-EPS-', 'the', 'some', 'a', 'an'])  # we will assume that `le` can be deleted\n",
    "lexicon['e'].update(['-EPS-', 'and', '&', 'also', 'as'])\n",
    "lexicon['chien'].update(['-EPS-', 'dog', 'canine', 'wolf', 'puppy'])\n",
    "lexicon['noir'].update(['-EPS-', 'black', 'noir', 'dark', 'void'])\n",
    "lexicon['blanc'].update(['-EPS-', 'white', 'blank', 'clear', 'flash'])\n",
    "lexicon['petit'].update(['-EPS-', 'small', 'little', 'mini', 'almost'])\n",
    "lexicon['petite'].update(['-EPS-', 'small', 'little', 'mini', 'almost'])\n",
    "lexicon['.'].update(['-EPS-', '.', '!', '?', ','])\n",
    "lexicon['-EPS-'].update(['.', 'a', 'the', 'some', 'of'])  # we will assume that `the` and `a` can be inserted\n",
    "print('LEXICON')\n",
    "for src_word, tgt_words in lexicon.items():\n",
    "    print('%s: %d options' % (src_word, len(tgt_words)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING INSTANCE: |x|=3 |y|=2\n",
      "le chien noir\n",
      "black dog\n",
      "\n",
      "D(x): 70 rules in 0.0092 secs or clean=70 rules at extra 0.0019 secs\n",
      "D(x,y): 52 rules in 0.0206 secs or clean=19 rules at extra 0.0014 secs\n",
      "19 loaded\n",
      "\n",
      "TRAINING INSTANCE: |x|=3 |y|=4\n",
      "le chien noir\n",
      "the black dog .\n",
      "\n",
      "D(x): 70 rules in 0.0054 secs or clean=70 rules at extra 0.0013 secs\n",
      "D(x,y): 94 rules in 0.0188 secs or clean=30 rules at extra 0.0008 secs\n",
      "30 loaded\n",
      "\n",
      "TRAINING INSTANCE: |x|=10 |y|=10\n",
      "le petit chien noir e le petit chien blanc .\n",
      "the little white dog and the little black dog .\n",
      "\n",
      "D(x): 707 rules in 0.0366 secs or clean=707 rules at extra 0.0089 secs\n",
      "D(x,y): 5347 rules in 0.2817 secs or clean=365 rules at extra 0.0221 secs\n",
      "365 loaded\n",
      "\n",
      "TRAINING INSTANCE: |x|=10 |y|=10\n",
      "le petit chien noir e le petit chien blanc .\n",
      "the little white dog and the little black dog .\n",
      "\n",
      "D(x): 707 rules in 0.0261 secs or clean=707 rules at extra 0.0067 secs\n",
      "D(x,y): 5347 rules in 0.2647 secs or clean=365 rules at extra 0.0431 secs\n",
      "365 loaded\n",
      "\n",
      "TRAINING INSTANCE: |x|=15 |y|=14\n",
      "le petit chien noir e le petit chien blanc e le petit petit chien .\n",
      "the little black dog and the little white dog and the mini dog .\n",
      "\n",
      "D(x): 2032 rules in 0.0654 secs or clean=2032 rules at extra 0.0195 secs\n",
      "D(x,y): 39447 rules in 1.7464 secs or clean=3411 rules at extra 0.2027 secs\n",
      "3411 loaded\n",
      "\n",
      "TRAINING INSTANCE: |x|=16 |y|=16\n",
      "le petit chien noir e le petit chien blanc e le petit chien petit blanc .\n",
      "the little black dog and the little white dog and the mini almost white dog .\n",
      "\n",
      "D(x): 2423 rules in 0.0606 secs or clean=2423 rules at extra 0.0227 secs\n",
      "D(x,y): 56642 rules in 2.5334 secs or clean=1716 rules at extra 0.2501 secs\n",
      "1716 loaded\n",
      "\n",
      "**** The next example should be out of the space of the constrained ITG ***\n",
      "TRAINING INSTANCE: |x|=20 |y|=20\n",
      "le petit chien noir e le petit chien blanc e le petit petit chien petit blanc e petit noir .\n",
      "the little black dog and the little white dog and the dog a bit white and a bit black .\n",
      "\n",
      "D(x): 4507 rules in 0.1130 secs or clean=4507 rules at extra 0.0452 secs\n",
      "D(x,y): 0 rules in 4.9806 secs or clean=0 rules at extra 0.0001 secs\n",
      "0 loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's test the faster parser!\n",
    "\n",
    "test(lexicon,\n",
    "        'le chien noir',\n",
    "        'black dog',\n",
    "        inspect_strings=False)\n",
    "test(lexicon,\n",
    "        'le chien noir',\n",
    "        'the black dog .',\n",
    "        inspect_strings=False)\n",
    "test(lexicon,\n",
    "        'le petit chien noir e le petit chien blanc .',\n",
    "        'the little white dog and the little black dog .')\n",
    "test(lexicon,\n",
    "        'le petit chien noir e le petit chien blanc .',\n",
    "        'the little white dog and the little black dog .')\n",
    "test(lexicon,\n",
    "        'le petit chien noir e le petit chien blanc e le petit petit chien .',\n",
    "        'the little black dog and the little white dog and the mini dog .')\n",
    "test(lexicon,\n",
    "        'le petit chien noir e le petit chien blanc e le petit chien petit blanc .',\n",
    "        'the little black dog and the little white dog and the mini almost white dog .')\n",
    "\n",
    "print('**** The next example should be out of the space of the constrained ITG ***')\n",
    "test(lexicon,\n",
    "        'le petit chien noir e le petit chien blanc e le petit petit chien petit blanc e petit noir .',\n",
    "        'the little black dog and the little white dog and the dog a bit white and a bit black .')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING INSTANCE: |x|=3 |y|=2\n",
      "le chien noir\n",
      "black dog\n",
      "\n",
      "Using LengthConstraint\n",
      "states=3\n",
      "initial=0\n",
      "final=0 1 2\n",
      "arcs=2\n",
      "origin=0 destination=1 label=-WILDCARD-\n",
      "origin=1 destination=2 label=-WILDCARD-\n",
      "D(x): 73 rules in 0.0060 secs or clean=73 rules at extra 0.0019 secs\n",
      "D_n(x): 262 rules in 0.0324 secs or clean=262 rules at extra 0.0057 secs\n",
      "D(x,y): 35 rules in 0.0079 secs or clean=14 rules at extra 0.0005 secs\n",
      "14 loaded\n",
      "\n",
      "TRAINING INSTANCE: |x|=3 |y|=4\n",
      "le chien noir\n",
      "the black dog .\n",
      "\n",
      "Using InsertionConstraint\n",
      "states=2\n",
      "initial=0\n",
      "final=0 1\n",
      "arcs=3\n",
      "origin=0 destination=0 label=-WILDCARD-\n",
      "origin=0 destination=1 label=-EPS-\n",
      "origin=1 destination=1 label=-WILDCARD-\n",
      "D(x): 73 rules in 0.0037 secs or clean=73 rules at extra 0.0014 secs\n",
      "D_n(x): 112 rules in 0.0076 secs or clean=112 rules at extra 0.0021 secs\n",
      "D(x,y): 192 rules in 0.0225 secs or clean=26 rules at extra 0.0013 secs\n",
      "26 loaded\n",
      "\n",
      "TRAINING INSTANCE: |x|=10 |y|=10\n",
      "le petit chien noir e le petit chien blanc .\n",
      "the little white dog and the little black dog .\n",
      "\n",
      "Using LengthConstraint\n",
      "states=11\n",
      "initial=0\n",
      "final=0 1 2 3 4 5 6 7 8 9 10\n",
      "arcs=10\n",
      "origin=0 destination=1 label=-WILDCARD-\n",
      "origin=1 destination=2 label=-WILDCARD-\n",
      "origin=2 destination=3 label=-WILDCARD-\n",
      "origin=3 destination=4 label=-WILDCARD-\n",
      "origin=4 destination=5 label=-WILDCARD-\n",
      "origin=5 destination=6 label=-WILDCARD-\n",
      "origin=6 destination=7 label=-WILDCARD-\n",
      "origin=7 destination=8 label=-WILDCARD-\n",
      "origin=8 destination=9 label=-WILDCARD-\n",
      "origin=9 destination=10 label=-WILDCARD-\n",
      "D(x): 668 rules in 0.0228 secs or clean=668 rules at extra 0.0077 secs\n",
      "D_n(x): 145688 rules in 5.8578 secs or clean=145688 rules at extra 1.5340 secs\n",
      "D(x,y): 6274 rules in 0.2808 secs or clean=562 rules at extra 0.0290 secs\n",
      "562 loaded\n",
      "\n",
      "TRAINING INSTANCE: |x|=10 |y|=10\n",
      "le petit chien noir e le petit chien blanc .\n",
      "the little white dog and the little black dog .\n",
      "\n",
      "Using InsertionConstraint\n",
      "states=4\n",
      "initial=0\n",
      "final=0 1 2 3\n",
      "arcs=7\n",
      "origin=0 destination=0 label=-WILDCARD-\n",
      "origin=0 destination=1 label=-EPS-\n",
      "origin=1 destination=1 label=-WILDCARD-\n",
      "origin=1 destination=2 label=-EPS-\n",
      "origin=2 destination=2 label=-WILDCARD-\n",
      "origin=2 destination=3 label=-EPS-\n",
      "origin=3 destination=3 label=-WILDCARD-\n",
      "D(x): 668 rules in 0.0137 secs or clean=668 rules at extra 0.0064 secs\n",
      "D_n(x): 9265 rules in 0.2543 secs or clean=9265 rules at extra 0.0937 secs\n",
      "D(x,y): 29179 rules in 2.6939 secs or clean=2182 rules at extra 0.1365 secs\n",
      "2182 loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now let's see how slow the previous way of parsing was\n",
    "\n",
    "from slow_example import test as slow_test\n",
    "\n",
    "slow_test(lexicon,\n",
    "        'le chien noir',\n",
    "        'black dog',\n",
    "        'length', inspect_strings=False)\n",
    "slow_test(lexicon,\n",
    "        'le chien noir',\n",
    "        'the black dog .',\n",
    "        'insertion', nb_insertions=1, inspect_strings=False)\n",
    "slow_test(lexicon,\n",
    "        'le petit chien noir e le petit chien blanc .',\n",
    "        'the little white dog and the little black dog .',\n",
    "        'length')\n",
    "slow_test(lexicon,\n",
    "        'le petit chien noir e le petit chien blanc .',\n",
    "        'the little white dog and the little black dog .',\n",
    "        'insertion', nb_insertions=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From here the length constrain is a bit too slow, but you can test if you are patient\n",
    "\n",
    "slow_test(lexicon,\n",
    "       'le petit chien noir e le petit chien blanc e le petit petit chien .',\n",
    "       'the little black dog and the little white dog and the mini dog .',\n",
    "       'length')\n",
    "slow_test(lexicon,\n",
    "        'le petit chien noir e le petit chien blanc e le petit petit chien .',\n",
    "        'the little black dog and the little white dog and the mini dog .',\n",
    "        'insertion', nb_insertions=3)\n",
    "slow_test(lexicon,\n",
    "       'le petit chien noir e le petit chien blanc e le petit petit chien petit blanc e petit noir .',\n",
    "       'the little black dog and the little white dog and the dog a bit white and a bit black .',\n",
    "       'length')\n",
    "slow_test(lexicon,\n",
    "        'le petit chien noir e le petit chien blanc e le petit petit chien petit blanc e petit noir .',\n",
    "        'the little black dog and the little white dog and the dog a bit white and a bit black .',\n",
    "        'insertion', nb_insertions=3)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
