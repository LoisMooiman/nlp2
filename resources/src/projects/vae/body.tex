In this project you will work with a neural extension of IBM model 1. 
You will learn how to marginalise discrete latent variables and you will also employ a continuous latent variable.

For the continuous case you will work with a variational auto-encoder formulation.


\section{Tasks}

\begin{description}
	\item[T1] We have prepared a notebook with theoretical background. You should read it carefully and answer a few questions.
	\item We have prepared a notebook with a tensorflow implementation of neural IBM 1. Your job is to extend that model 
	\begin{description}
		\item[T2] Neural IBM 1 with additional French context (4.2 and 4.3 in the notebook). You complete this task by showing us a plot of likelihood (training/dev) and AER (dev/test) per epoch.
		\item[T3] Neural IBM 1 with collocations (5.1 in the notebook). You complete this task by showing us a plot of likelihood (training/dev) and AER (dev/test) per epoch.
		\item[T4] Neural IBM 1 with latent gate (6.x in the notebook). You complete this task by showing us a plot of likelihood (training/dev) and AER (dev/test) per epoch.
	\end{description}
\end{description}

\section{Report}

Instead of a report, we expect a link to a github repository containing one notebook for each task. 

\section{Assessment}

Task 1 is worth 4 points. Tasks 2, 3 and 4 are worth 2 points each. 