In this project you will work with a neural extension of IBM model 1. 
You will learn how to marginalise discrete latent variables and you will also employ a continuous latent variable.

For the continuous case you will work with a variational auto-encoder formulation.


\section{Tasks}

\begin{description}
	\item[T1] We have prepared a notebook with theoretical background. You should read it carefully and answer a few questions.
	\item We have prepared a notebook with a tensorflow implementation of neural IBM 1. Your job is to extend that basic model. 
	\begin{description}
		\item[T2] Neural IBM 1 with additional French context (Section 2.1 of the notebook). You complete this task by showing us a plot of likelihood (training/dev) and AER (dev/test) per epoch.
		\item[T3] Neural IBM 1 with collocations (Section 2.2 of the notebook). You complete this task by showing us a plot of likelihood (training/dev) and AER (dev/test) per epoch.
		\item[T4] Neural IBM 1 with latent gate (Section 2.3 of the notebook). You complete this task by showing us a plot of likelihood (training/dev) and AER (dev/test) per epoch.
	\end{description}
\end{description}

\paragraph{Note that} ~ You are free to use a framework other than tensorflow, but then our support may be limited by your choice. You can also use pre-defined layers (e.g. MLPs, Embedding layers, etc.) and optimisers (e.g. SGD, Adagrad, Adam), but note that you still need to answer theoretical questions about some of these.
 

\section{Report}

Instead of a report, we expect a link to a github repository containing one notebook for each task. 

\section{Assessment}

Task 1 is worth 4 points. Tasks 2, 3 and 4 are worth 2 points each. 
